{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17) Data in Space - Networks <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 15 taught us concurrency: how to do more than one thing at a time. Now we will try to do things in more than one place: distributed computing or networking. There are many good reasons to do this:\n",
    "\n",
    "    Performance\n",
    "        Your goal is to keep fast components busy, not waiting for slow ones.\n",
    "        \n",
    "    Robustness\n",
    "        There is safety in numbers, so you want to duplicate tasks to work around hardware and software failures.\n",
    "        \n",
    "    Simplicity\n",
    "        It is best practice to break complex tasks into many little ones that are easier to create, understand and fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networking patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build networking applications from some basic patterns:\n",
    "\n",
    "    The most common pattern is request-reply (client-server). This pattern is synchronous: the client waits until the server responds. A web browser is a client, making an HTTP request to a web server ,which returns a reply.\n",
    "    \n",
    "    Another common pattern is push or fanout: you send data to any available worker in a pool of processes. An example is a web server behind a load balancer.\n",
    "    \n",
    "    The opposite of push is pull or fanin: you accept data from one or more sources. An example would be a logger that takes text messages from multiple processes and writes them to a single log file.\n",
    "    \n",
    "    One pattern is similar to television broadcasting: publish-subscribe. With this pattern, a publisher sends out data. In  a simple pub-sub system all subscribers would receive a copy. More often, subscribers can indicate that they are interested only in certain types of data, and the publisher will send just those. Unlike the push pattern, more than one subscriber might receive a given piece of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publish-Subscribe pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish-Subscribe is not a queue but a broadcast. One or more processes publish messages. Each subscriber process indicates what type of messages it would like to receive. A copy of each message is sent to each subscriber that matched its type. Thus, a given message might be processed once, more than once, or not at all. The Redis package contains a pub-sub system. The publisher emits a message with a topic and a value, and subscribers say which topics they want to receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a publisher that broadcasts letters and numbers\n",
    "\n",
    "def publisher():\n",
    "    conn = redis.Redis()\n",
    "    letters = ['A', 'B', 'C', 'D']\n",
    "    numbers = [1, 2, 3, 4]\n",
    "    \n",
    "    for msg in range(10):\n",
    "        letter = random.choice(letters)\n",
    "        number = random.choice(numbers)\n",
    "        print('Publish: {} and {}'.format(letter, number))\n",
    "        conn.publish(letter, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a subscriber that is only interested in certain letters\n",
    "\n",
    "def subscriber():\n",
    "    conn = redis.Redis()\n",
    "    topics = ['B', 'D']\n",
    "    sub = conn.pubsub()\n",
    "    sub.subscribe(topics)\n",
    "    \n",
    "    for msg in sub.listen():\n",
    "        if msg['type'] == 'message':\n",
    "            letter = msg['channel']\n",
    "            number = msg['data']\n",
    "            print('Subscribe: {} and {}'.format(letter, number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the subscriber wants all messages for letters B and D and no others. The listen() method returns a dictionary. If its type is 'message', it was sent by the publisher and matches our criteria. The 'channel' key is the topic (letter), and the 'data' key contains the message (number). You can have as many subscribers and publishers as you want. If there is no subscriber for a message, it disappears from the Redis server. However, if there are subscribers, the messages stay in the server until all subscribers have retrieved them. To run servers and clients, start the server and client in separate terminal windows or put the server in the background with a final \"&\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web services and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is published only on a website, anyone who wants to access and structure the data needs to write scrapers and rewrite them each time a page format changes. In contrast, if a website offers an API to its data, the data becomes directly available to client programs. APIs change less often than web page layouts, so client rewrites are less common. APIs are especially useful for mining well-known social media sites such as Twitter, Facebook and LinkedIn. All these sites provide APIs that are free to use, but they require you to register and get a key to use when connecting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote management tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Google and other internet companies grew, they found that traditional computing solutions did not scale. Software that worked for single machines, or even a few dozen, could not keep up with thousands. Disk storage for databases and files involved too much seeking, which requires mechanical movement of disk heads. But you could stream consecutive segments of the disk more quickly. Developers found that it was faster to distribute and analyze data on many networked machines than on individual ones. Big data often means \"data too big to fit on my machine\": data that exceeds the disk, memory, CPU time or all of the above. A selection of packages for remote management include:\n",
    "\n",
    "    - Hadoop\n",
    "    - Spark\n",
    "    - Disco\n",
    "    - Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so long ago, you would buy your own servers, bolt them into racks in data centers and install layers of software on them: operating systems, device drivers, filesystems, databases, web servers, email servers, name servers, load balancers, monitors and more. Many hosting services offered to take care of your servers for a fee, but you still leased the physical devices and had to pay for your peak load configuration at all times. Instead of building, you can rent servers in the cloud. By adopting this model, maintenance is someone else's problem and you can concentrate on your service. Using web dashboards and APIs, you can spin up servers with whatever configuration you need, quickly and easily. You can monitor their status and be alerted if some metric exceeds a given threshold. The big cloud vendors are:\n",
    "\n",
    "    - Amazon (AWS) (pip install boto3)\n",
    "    - Google\n",
    "    - Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Containers are much lighter than virtual machines and a bit heavier than Python virtualenvs. They allow you to package an application separately from other applications on the same machine, sharing only the operating system kernel. To install Docker's Python client library run pip install docker. Containers caught on and spread through the computing world. Eventually, people needed ways to manage multiple containers and wanted to automate some of the manual steps that have been usually required in large distributed systems, including failover, load balancing and scaling up and down. Kubernetes is leading the pack in container orchestration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
